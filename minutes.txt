+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=
where slides are found - 6

past lecture quick review - 8
	why computer vision is difficult, list of challenges computers face - 11
		stereo vision spiel - 14
		context - 16

syllabus covering - 18
	"let me know if you're interseted in learning more, i can change the syllabus" - 22
	"final will be easy" , final project - 23

"have you heard of these before?" - 32

images and filters - 35
	what is an image? - 35
		rgb, grayscale - 36
		tensor, matrix, array, scalar, cubes, dimensions - 37
		an image is a cube/3d tensor (3rd dimension is color/grayscale) - 38

	images as functions - 38

	image operations - 40
			"assume everything is grayscale for this lecture" - 42
		edge detection - 43
		converting image to vector - 43
		comparing two images - 44
		denoising - 45
		
	image filtering/convolution - 46
		g-matrix - 47
		blurring/smoothing - 49
			what does it mean to convolution/convolving? - 50
			explaining the formula - 52
			handling the border of the image (0 padding) - 53
		box filter - 57
			why choose 1/9 - 57

		this filter does nothing - 59
				einsteins eye - 1.00
		left shift filter - 1.01
		sharpening filter / brightness subtracted by smoothing - 1.01
		edge detection filter vertical and horizontal - 1.05
		
		gaussian filter - 1.09
			what does the standard deviation do? variance? - 1.15
			gaussian vs box - 1.16
			combining gaussian filters - 1.18
			seperable filters are more efficient - 1.19		
+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=
exams and cheatsheets - 5

filters review - 8
	discretizing meaning and why we do it - 8
	pixels, cameras, RGB, displays - 9
	operations - 10
		image filtering - 11
		why our eyes our blurry, pinhole cameras, pupils & retinas - 12
			early cameras - 14
				lenses - 20
			why our eyes are blurry - 22
		other filters, edge detection, gaussian - 24

box filter algorithm / sums of rectangular regions - 26
	calculating the sum more efficiently  - 28
	"this way is efficient but its not the best" - 31
	the best way - 32
		cant be done with gaussian, and their is a big 1-time cost  - 34

addressing constant blur to remove noise without removing detail - 35
	bilateral filter on a height field - 37
		theory and general math behind it - 39
			"big differences will have much less weight... the edge will not be smoothed out" - 40
			"this is not convolution anymore" - 42
		explaining and going deeper into math - 42
		the result of the bilateral filtering - 45
			"its not a convolution" - 48

AI image generation - 49

borders - 51

sampling - 53
	image scaling / sub-sampling / size reduction - 54
		sub sampling by using every other pixel value - 55
			interpolation/averaging - 55
			the quality drops a lot, information is lost, artifacts are introduced - 57
		sub sampling with gaussian pre-filtering introduction - 58

	fourier transform - 59
		waves visualized inside the fourier domain - 1.09
		jpg and mp3 file compression and how our ears hear - 1.11
			Nyquist and why things are sampled at 44khz when we can only hear 22khz - 1.16
			the old phone system's limitations  - 1.19
+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=
past lecture recap - 6
	artifacts - 6
	sub sampling - 7
	gaussian pre-filtering - 8
	fourier transform - 9
	nyquist rate sampling & aliasing - 10
		avoiding aliasing - 13

	using fourier to make an image(i think?) - 17
		fourier holds in any dimension - 17

	more on subsampling - 23
		sampling too much? - 25
		phone sample example - 27

what aliasing looks like - 29
	helicopter, car wheel example - 32
	"this is because of UNDERsampling, oversampling has no effect" - 36
		
upsampling - 37
	upsampling methods (nearest neighbor, bilinear, bicubic) - 38
	superresolution / ML for upsampling, its on your phone - 43
	
oversampling downsides - 44

edge detection - 45
	recognition is very related to edge detection - 45
	some neurons are designed for edge detection - 47
	why mimicking humans isnt ideal - 49

	what makes an edge - 50
		discontinuity (surface normal, depth, surface color, illumination) - 51
		
	finding edges using algorithms - 53
		1st, 2nd derivatives - 54

	image gradient, derivative for an image/vector / high dimension derivatives - 55
		partial derivative - 56
		"its always pointing at the whiter part" - 1.00
		
		calculate derivative of discrete/non-continuous signal - 1.03
	
	sobel operator - 1.06
	noise vs derivatives - 1.11
		Derivative Theorem of Convolution - 1.12
		canny edge detection, orientation of an edge - 1.16
+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=
edge detection recap - 7
	saddle point of derivative - 13
	sobel filter - 15
	Derivative Theorem of Convolution - 17

canny edge detection - 19
	hough space - 24

intro to machine learning - 33
	recognition function, y=f(x) - 34
	training vs testing - 36
		"we dont want to memorize" - 37
			self driving car example - 40

	steps to machine learning - 42
	efficiency & power consumption & edge AI - 45

	feature extraction, turning an image into a vector - 48
		some possible "obsolete" features - 50
			deeplearning makes it obsolete (end to end) - 53

	classification - 54
		nearest neighbor - 54
			simple but slow - 56
		linear - 57
		decision trees / 20 questions - 1.00

	supervision , annotation - 1.02
		spectrum of supervision - 1.07

	generalization vs memory - 1.11
	diagnosing generization ability / a model's ability - 1.13
		overfitting & underfitting - 1.13
			freedom of a model - 1.14
			cross validation - 1.18
			"if you increase the size of the model, you need more data to accomodate" - 1.21
+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=
nothing today
+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=
turning friday discussion into lecture & vice versa - 10
hw1 restrictions & logistics - 11
	late day policy - 11
	prebuilt libraries - 12
	what to submit - 17
	which submission counts - 18
	
machine learning review - 19
	knobs / parameters - 20
	learnable function - 21
	supervised learning - 21
	training and testing steps - 23
	classifiers - 24
		nearest neighbor - 24
		linear - 26
			what is a parametric model - 26
	types of supervision - 27
	generalization vs memory - 29
		this set is "held out" - 29
	undefitting , overfitting - 30
	model complexity & training size & test error - 35
		more data = bigger models - 36
	validation , choosing the best model - 37
		"we dont prefer the model on the test set a lot..." - 37

midterm in early november - 42

data set history - 43

neural networks - 53
	"AI nowadays are neural networks" - 54
	how binary classifier works: variables, weights, threshold - 54
		why we transpose - 56
		why we add bias into our feature matrix - 57
	spam example - 58
		"whats a word that should have negative weight" - 1.00
		"whats a word that should have zero weight?" - 1.01
				*mic is out* - 1.01
				*mic is back* - 1.02
	geometry visualization - 1.04
		spam example - 1.04
		new example - 1.08
			"why is it 90 degrees?" - 1.09
		hyperplane - 1.10

	two-layer network architecture - 1.12
		"linear models can't solve every problem", linearly non-seperable , non-linear model - 1.13
		deep model - 1.13		
+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=
you dont need permission for late days - 10

previous lecture review - 12
	perceptron - 12
	geometric visualization - 15
	
MLP multi-layer perceptron - 17
	non-linearity / link function / activation function - 19
		we want derivable functions - 21
	hidden layers / hidden neurons - 22
		more neurons = more capacity - 23
		depth vs width - 25
		finding the sweet spot, over-training - 25
			"i can always use SQL for 100%!" - 25
	temperature prediction example - 26
		ground truth - 27
		cost function, objective function, loss function, energy function, error function - 27
				machine learning nowadays, "optimization" - 31
					UPS/FEDEX navigation directions example "never did left turns" - 31
			using absolute value vs squaring - 34
				lasso - 38

optimization framework, and loss minimization - 38
	argument / index of array - 40
	w, freedom - 40
	why we use 1/2 - 43
	regularization - 45

"How can we optimize?" - 46
gradient descent - 47
	most used and easy to understand - 47
	"its just the derivative" - 49
	gradients & partial derivative - 50
		gradient = vector of partial derivatives - 53
	"derivative calculates going up the hill, but we want to go down the hill..." - 55
	step size / learning rate, local minima - 56

ADAM gradient descent & automatic learning rate - 1.00

MLP multi layer perceptron - 1.01

gradient descent update rule / automatic learning rate - 1.02

"how to calculate derivatives?" - 1.04
	chain rule review - 1.08
	bringing it back into MLPs - 1.11
		"this is time consuming" - 1.11
	back propogation and the chain rule - 1.12
		example - 1.14
		forward pass - 1.20		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=
previous lecture review - 7
	hw answers will be provided - 7
	linear classifier - 7
	multi layer perceptron - 8
	non-linearity / activation / link function - 9
	optimizing the number of hidden layers - 10
		optimization framework - 10
		the 2 arguments - 12
	gradient descent algorithm - 12
		gradient vector / partial derivative vector - 14
		learning rate, ADAM - 18
		
momentum, step sizes & inertia, avoiding local minima - 21
	"the number of minimas is infinite" & little ant analogy - 28
		alchemey and black magic - 29
				
optimization is slow - 33
	imageNET dataset, resNET take 20 hours to calculate - 33
	stochastic gradient / minibatch gradient, "i dont need to use all my training data" - 34
		random sampling - 36
		mini-batch & stochastic definitions - 37
	IID independentent identically sampled - 38
	EPOCH - 39
	HDD harddrives are slow, ram is fast - 42
	
gradient descent quick refresh - 48 
	forward pass backward pass -  53
	
two types of compute - 55

history of back propogation algorithm - 1.00
	auto differentiation, facebook pytorch, google tensorflow. numpy - 1.03

initializing - 1.05
	multiple solutions = #(hidden neurons)_Factorial - 1.06
	infinite number of local minima - 1.09
	
DAG directed acyclic graph - 1.11

breadth vs depth, vanishing gradient exploding gradient - 1.12

CPU vs GPU - 1.16
		*mic goes out* - 1.17
		*mic comes back* - 1.18
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=
no numerical examples for gradient descent - 6
hw2, gradient descent - 7

previous lecture review - 8
	dynamic programming - 9
	momentum - 9
	
activation functions - 12
	why dont we use sign function - 12
		sign vs step vs sine (lol whoever asked, thank you) - 14
	sigmoid, tan, and vanishing gradient, saturation - 15
	ReLu function, rectified linear unit - 17
		but derivative = 0 here - 18
			leaky ReLu - 18
				"doesnt help much" - 19
		ReLu is better than sigmoid - 19
			"what about this non-differentiable point?", engineer vs math approach - 20
			
convolutional neural networks CNN - 22
	end to end learning - 23
	MLP 3d weight visualization - 26
	
tensorflow playground - 27
	linear cannot solve this problem - 29
	small,large learning rate - 31
	epoch - 32
		"usually we do 50 epoch, each epoch takes 20 minutes" - 32
	large step sizes - 34
		"it converges but still fluctuates" - 34
		"theres no benefit to large step sizes" - 34
	"how do you know you're done?" - 35
		there is automation but todays we mostly use babysitting - 36
	ReLu vs sigmoid - 36
		ReLu is a polygonal, and it can never smooth out the corners - 37
	regularization, L1, L2, absolute value, lasso, sparse - 38
		L1 vs L2 - 41
			lambda regularization rate - 43
			capacity reduction - 46
				sparse - 47
	adding hidden layers, neurons - 50
		"if its failing on training, the model is lacking capacity" - 51
		breadth vs depth, "more layers is better but problematic" - 51
	adding input - 52
	why the model isnt a perfect spiral - 53
		*mic out* - 53
		*mic in* - 55
	classification vs regression/prediction - 55
		"sometimes extra input can be confusing" - 57
	adding noise - 57
	bach size, small vs large - 58
		bach size and training time - 1.00
			mini bach has more updates which is better - 1.00
			cons of mini bach - 1.03
		even though this one is 20k+ epochs to solve, its because our input is small - 1.06
			half epochs - 1.06

CNN is an MLP with convolutions instead of linear layers - 1.09
	tensor review - 1.09
	weight mask, image, feature map, channels, convolutional layer, fully connected, & calculating their sizes - 1.10 
	feature extraction - 1.16
		low level, mid level, high level - 1.19
	flow of CNN - 1.19
		spatial pooling, max pooling, stride - 1.21
+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 30 thursday +=+=+=+=+=+=+=+=
swift coding club - 6

past lecture review - 8
	combining convolutions while avoiding linearity - 9
	ReLu, avoiding saturation - 11
	spatial pooling, stride, "reduce the amount of info we pass forward" - 18
	
boat cnn example - 21
	every layer has an activation after it - 23
	fully connected meaning - 24
	long vector dimension, vectorization - 24
	softmax activation function, probability distribution - 25
	"how i analyze a research paper" - 30
	1-hot encoding - 31
	dimension of tensor,vector - 32
			"maybe i should stop recording" - 35
	calculating loss - 38
	"it must be differentiable" - 44
	non differentiable and reinforcement learning - 45
	not using fully connected for convolution, "detect the cat anywhere in the image" - 49
	
SIFT, AlexNet, deep learning, end to end, & history - 51
	LeNet digit detection - 53
	AlexNet and using GPU - 55
	AlexNet changes the game - 1.04
	
looking inside what the model sees, explainable AI, top 9 patches - 1.09
	layer 1 - 1.09
		how to see the top filters - 1.10
	layer 2 - 1.14
	layer 3 - 1.15
	layer 4 - 1.16
		"why are there so many dogs?" - 1.16 
	layer 5 - 1.17
	why is explainable AI important, medical - 1.20
	
error top 5, top 5 error - 1.22
+=+=+=+=+=+=+=+= 10 - 30 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 4 tuesday +=+=+=+=+=+=+=+=
max pooling stride - 7
	its still used today even though it doesnt improve speed - 11
	
final project proposal - 12

midterm, "dont worry about it" - 13	

previous lecture review - 14
	top 9 patches , what the model sees - 14
	tweaking pre-built models - 19
	early exit - 19
	"why would we want to reduce size?", edge devices, edge computing - 21
	
imageNet challenge - 22
	top 5 error - 24
	"human expert is stupid" - 27
	comptetition cheating - 29
			*mic out* - 32
			*mic in* - 33
			
Resnet "going beyond 22 layers was difficult"  - 34
	"if one layer doesnt work, the chain breaks" - 36
	skip connection - 37
	average pooling - 39
	"why not one connection from the input all the way to the output - 42
	resnet block - 42
	
	DenseNet - 43
	Neural Architectural Search, NAS, AI improving model architecture - 44
	
	"why not a connection for every block instead of every other block?" - 45
	skip connections are differentiable - 46
	effective depth - 48
	plain network vs resnet comparison - 50
	
BN batch normalization, handling the constant changing of layer inputs/outputs - 52
	"normalizing the distribution" - 55
	its several times faster - 55
	parameters mew sigma / gamma beta - 58
		"you can take this extra stuff out and be fine" - 1.00
	"Do i need to keep this for the final model?", baking in parameters into the model weight and bias - 1.01

deep learning packages & history: cafe, cuda, torch, lua - 1.08
	cuda, nvidia, amd - 1.16
		how companies get more customers - 1.18
	
coding on the midterm - 1.20
	double sided hand written cheatsheet - 1.22
+=+=+=+=+=+=+=+= 11 - 4 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 6 thursday +=+=+=+=+=+=+=+=
past lecture review - 4
	top-9 patches, imagenet challenge - 4
	batch norm - 5
		layer norm instead of batch norm - 7
	resnet - 9
	deep learning packages, pytorch - 10
	
adversarial attack, breaking CNNs - 12
	designing adversarial attack - 13
		black box, white box - 15
	targeted vs no target attack - 17
	modified gradient, gradient ascent "going up the hill not down the hill" - 18
	
	"this isnt practical", adversarial patch - 19
		hillary clinton glasses - 24
		defense against adversarial patch - 27
		
	simple solution, add adversarial images to training, adversarial training, adversarial example - 28
		"its not the best solution" - 30
		*mic out* - 30
		*mic in* - 31
	sharpness aware minimization - 31

data augmentation (cropping, scale, flipping, angling...) - 36
	"if you dont do augmentation in DNN, you will overfit" - 39
	pytorch & augmentation - 40
	we dont test on augmented images - 41
	be careful of augmentation leaking from training to testing - 41
	
	augmentation is domain specific - 42
			augmentation in test time, 10crop - 43
		examples of when augmentation can go wrong,right - 43
		
dropout - 45
	dont use it during test time - 48
	pruning, permanently removing inactive neurons - 49
	drop connect - 50
	
	model ensemble, "why are there 30 people playing the violin when 1 would be fine?" - 51

large datasets dont use augmentation - 55

"augmentation should be done on the fly" - 56

transfer learning, building models off small data sets - 57
	frozen layers - 58
	how it works, "feature learned through imagenet may be sufficient..." - 59
	
	the initial data set should be diverse, imagenet has many applications - 1.01
	
	"this visualization isnt that great" - 1.03
	
	the more data you have, the more of the model you train - 1.04
		fine tuning - 1.05
	pre training - 1.08
	CNN comparison - 1.09
	
	transfer learning for image captions, open vocabulary, closed vocabulary - 1.11
	
parallel computation, hadoop, mapreduce - 1.14
	gpu failure - 1.18
model parallel, weight parallel, data parallel - 1.22
+=+=+=+=+=+=+=+= 11 - 6 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 11 tuesday +=+=+=+=+=+=+=+=
verterans day, nothing
+=+=+=+=+=+=+=+= 11 - 11 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 13 thursday +=+=+=+=+=+=+=+=
midterm, nothing
+=+=+=+=+=+=+=+= 11 - 13 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 18 tuesday +=+=+=+=+=+=+=+=
"how was the exam? :)" - 8
	final will be similar - 8

past lecture review - 8
	data augmentation - 9
		bad augmentation - 9
	transfer learning - 12
			augmentation hurts early epochs - 13
		fine tuning - 15
			pre train - 16 
	GPU training - 19
	
skip homework 3 or have homework 3 - 21

GAN , generative adversarial learning , generative adversarial network - 24
	random noise to generate a natural image - 26

	Generator, Discriminator - 28
		generator - 28
			up convolution - 28
		game theory - 31
		discriminator - 31
		training data vs fake data switch - 32
		
		training G, D - 32
			"its a game" - 34
					"its two layers appended to eachother" - 35
			one layer maximizing loss, one layer minimizing loss - 35	
		G and D have to have good balance - 37
		discriminator may be used after training for - 39
			use D as generative AI detector - 39
					*mic out* - 39
					*mic in* - 40
				feeding AI to train a model is bad - 41
					watermarking - 41
			use D for transfer learning - 42
					
	Manifold space, latent space data space, where natural images live - 43
	
	GAN training car example - 47
	
	GAN training: local minima & local maxima at the same time, saddle point - 49
		"its more difficult" - 51
		
	video GAN - 53
		example videos - 58
		humans are sensitive to faces - 1.00
		the first frame should look like the input image - 1.02
		
applications of GAN - 1.05
	generating pictures - 1.05
	image to image translation - 1.08
		cycle consistency - 1.09
	text to image - 1.11
	semantic image to photo translation - 1.13
		creating simulations for self driving cars - 1.15
	more image to image - 1.17
	
we will talk about diffusion model later - 1.21
+=+=+=+=+=+=+=+= 11 - 18 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 20 thursday +=+=+=+=+=+=+=+=
previous lecture review - 4
	GAN generative adversarial network recap - 5
	
	manifold space - 6
		3d probability distribution - 8
		learning distribution through sampling (genAI does this) - 9
		
		the early GPT model, text completion - 16
		
	GAN training - 21
		latent vector - 23
		
diffusion models - 26
	turing test - 27
	
	forward diffusion, adding noise - 28
	backward diffusion, removing noise, "each time it gets closer to the blanket" - 30
	"if its not on the manifold its because noise has been added" - 32
	most genAI models (dalle) are diffusion based - 32
		no latent space - 34
			CFG classifier free guidance  - 35
	we tell the model the type of noise not the amount of noise - 39
	
	LADA LLaDA large language diffusion models - 42
	
	super resolution - 44
		why it shouldnt be used in medical - 46
	
	image to image translation - 49
	3d shape asset generation - 50
	
	non conditional model "its a natural image but it doesnt really help me" - 55
	
object detectoin and image segmentation - 56
	classification vs semantic segmentation vs object detection vs instance segmentation - 56

	semantic segmentation - 59
		sliding window method "its too expensive" - 59
		convolution method - 1.01
		fully convolution method, hour glass - 1.02
			upsampling, unpooling, max unpooling - 1.04
			fractional stride - 1.08
			
	object detection - 1.11
		single object detection - 1.11
		why we do multitask loss instead of two models, shared learning - 1.16
		mutliple object detection - 1.21
			sliding window - 1.21
			region proposal generation rcnn, object nest - 1.22
+=+=+=+=+=+=+=+= 11 - 20 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 25 tuesday +=+=+=+=+=+=+=+=
midterm, hw3, gpu acess - 4

previous lecture recap - 10
	classification vs semantic segmentation vs instance segmentation - 11
	semantic segmentation - 11
			transformers - 13
	instance segmentation - 15
		shared learning, multitask learning, multitask loss - 18
		detecting multiple objects - 22
			precision & recall - 23

rcnn  - 25
	history - 25
	rcnn - 30
	fast rcnn - 32
	faster rcnn - 38
	
SSD, single shot object detection, single stage object detection  - 40
	YOLO you only look once - 40
	backbone, meta architecture, image size - 47
		"bigger backbones work better" - 48
	mask rcnn - 50
		joint prediction - 53
		
scene graph - 58
3d object detection - 1.01

depth detection , depth estimation - 1.06
	multiple cameras, ML method, lidar/radar, sonar, ToF time of flight - 1.07
	
	lidar cloud points as input into NN - 1.12
		pointNet - 1.13
		xbox kinect and ToF - 1.16
		
stereo vision - 1.19
+=+=+=+=+=+=+=+= 11 - 25 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 27 thursday +=+=+=+=+=+=+=+=
nothing. thanksgiving holiday
+=+=+=+=+=+=+=+= 11 - 27 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 12 - 02 tuesday +=+=+=+=+=+=+=+=
previous lecture recap - 6
	multiple object detection, sliding window, object nest region proposal - 7
	rcnn fast rcnn faster rcnn - 8
	mask rcnn - 11
	open vocabulary - 11
	
RNN Recurrent Neural Network - 13
	one to one, one to many, many to one, many to many - 17
	hidden state - 21
		example - 24
	you can infinite number of layers - 25
	training, "what is the problem with back propogation?" - 27
	
	sequence to sequence, language translation - 32
		encoder decoder - 39
		encoder text to speech - 42
		
	various question answering - 43
	
	example training - 49
			*mic out* - 52
			*mic in* - 53
			
	temperature - 55
	
	embedding matrix embedding layer embedding vector - 59
	
	truncated back propogation, window based loss - 1.05
	
	min-char-rnn.py - 1.09
	
	shakespeare, textbook, C, example example - 1.10
	
	interpretable cells, emerging properties - 1.18
+=+=+=+=+=+=+=+= 12 - 02 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 12 - 04 thursday +=+=+=+=+=+=+=+=
midterm and final - 6

previous lecture recap - 8
	variable length input sequences, output sequences - 9
	sequence to sequence learning, encoder decoder - 10
	truncated back propogation - 13
	interpretable cells, emerging properties - 14
		room vs object detection emerging behavior - 15	

pros and cons of RNN, RNN tradeoffs, slow, cant parallelize - 19

image captioning, cnn + rnn - 21
		token - 22
		
model failing - 27
	corner cases, tail distribution, not enough input, women surfboard example - 27
	VQA Visual Question Answering, dataset bias - 29
	
LSTM long short term memory - 33

"fill out the course evaluation" - 34

attention model - 44
	context - 44
	attention - 46
	how to create output - 46	
	english to french visualization example - 53
		"in practice it doesnt look this clean" - 57
		
	the order doesnt matter, attention for image captioning - 58

	bird visualation example - 1.04
	
general attention layer - 1.07
	query, keys, value - 1.09
	
self attention - 1.11
	position embedding, position encoding - 1.14
	causal language, causal attention - 1.15
	
transformer, transformer encoder transformer decoder - 1.17
	image captioning using transformer, VIT Vision transformer - 1.19
	"use can input any medium!" - 1.20
	transformers dont see sequences - 1.21
+=+=+=+=+=+=+=+= 12 - 04 thursday +=+=+=+=+=+=+=+= 

================================================================================================================================
================================================================================================================================

that is all !

( ╥﹏╥) ノシ
