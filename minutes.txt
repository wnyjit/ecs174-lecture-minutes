+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=
where slides are found - 6

past lecture quick review - 8
	why computer vision is difficult, list of challenges computers face - 11
		stereo vision spiel - 14
		context - 16

syllabus covering - 18
	"let me know if you're interseted in learning more, i can change the syllabus" - 22
	"final will be easy" , final project - 23

"have you heard of these before?" - 32

images and filters - 35
	what is an image? - 35
		rgb, grayscale - 36
		tensor, matrix, array, scalar, cubes, dimensions - 37
		an image is a cube/3d tensor (3rd dimension is color/grayscale) - 38

	images as functions - 38

	image operations - 40
			"assume everything is grayscale for this lecture" - 42
		edge detection - 43
		converting image to vector - 43
		comparing two images - 44
		denoising - 45
		
	image filtering/convolution - 46
		g-matrix - 47
		blurring/smoothing - 49
			what does it mean to convolution/convolving? - 50
			explaining the formula - 52
			handling the border of the image (0 padding) - 53
		box filter - 57
			why choose 1/9 - 57

		this filter does nothing - 59
				einsteins eye - 1.00
		left shift filter - 1.01
		sharpening filter / brightness subtracted by smoothing - 1.01
		edge detection filter vertical and horizontal - 1.05
		
		gaussian filter - 1.09
			what does the standard deviation do? variance? - 1.15
			gaussian vs box - 1.16
			combining gaussian filters - 1.18
			seperable filters are more efficient - 1.19		
+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=
exams and cheatsheets - 5

filters review - 8
	discretizing meaning and why we do it - 8
	pixels, cameras, RGB, displays - 9
	operations - 10
		image filtering - 11
		why our eyes our blurry, pinhole cameras, pupils & retinas - 12
			early cameras - 14
				lenses - 20
			why our eyes are blurry - 22
		other filters, edge detection, gaussian - 24

box filter algorithm / sums of rectangular regions - 26
	calculating the sum more efficiently  - 28
	"this way is efficient but its not the best" - 31
	the best way - 32
		cant be done with gaussian, and their is a big 1-time cost  - 34

addressing constant blur to remove noise without removing detail - 35
	bilateral bilter on a height field - 37
		theory and general math behind it - 39
			"big differences will have much less weight... the edge will not be smoothed out" - 40
			"this is not convolution anymore" - 42
		explaining and going deeper into math - 42
		the result of the bilateral filtering - 45
			"its not a convolution" - 48

AI image generation - 49

borders - 51

sampling - 53
	image scaling / sub-sampling / size reduction - 54
		sub sampling by using every other pixel value - 55
			interpolation/averaging - 55
			the quality drops a lot, information is lost, artifacts are introduced - 57
		sub sampling with gaussian pre-filtering introduction - 58

	fourier transform - 59
		waves visualized inside the fourier domain - 1.09
		jpg and mp3 file compression and how our ears hear - 1.11
			Nyquist and why things are sampled at 44khz when we can only hear 22khz - 1.16
			the old phone system's limitations  - 1.19
+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=
past lecture recap - 6
	artifacts - 6
	sub sampling - 7
	gaussian pre-filtering - 8
	fourier transform - 9
	nyquist rate sampling & aliasing - 10
		avoiding aliasing - 13

	using fourier to make an image(i think?) - 17
		fourier holds in any dimension - 17

	more on subsampling - 23
		sampling too much? - 25
		phone sample example - 27

what aliasing looks like - 29
	helicopter, car wheel example - 32
	"this is because of UNDERsampling, oversampling has no effect" - 36
		
upsampling - 37
	upsampling methods (nearest neighbor, bilinear, bicubic) - 38
	superresolution / ML for upsampling, its on your phone - 43
	
oversampling downsides - 44

edge detection - 45
	recognition is very related to edge detection - 45
	some neurons are designed for edge detection - 47
	why mimicking humans isnt ideal - 49

	what makes an edge - 50
		discontinuity (surface normal, depth, surface color, illumination) - 51
		
	finding edges using algorithms - 53
		1st, 2nd derivatives - 54

	image gradient, derivative for an image/vector / high dimension derivatives - 55
		partial derivative - 56
		"its always pointing at the whiter part" - 1.00
		
		calculate derivative of discrete/non-continuous signal - 1.03
	
	sobel operator - 1.06
	noise vs derivatives - 1.11
		Derivative Theorem of Convolution - 1.12
		canny edge detection, orientation of an edge - 1.16
+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=
edge detection recap - 7
	saddle point of derivative - 13
	sobel filter - 15
	Derivative Theorem of Convolution - 17

canny edge detection - 19
	hough space - 24

intro to machine learning - 33
	recognition function, y=f(x) - 34
	training vs testing - 36
		"we dont want to memorize" - 37
			self driving car example - 40

	steps to machine learning - 42
	efficiency & power consumption & edge AI - 45

	feature extraction, turning an image into a vector - 48
		some possible "obsolete" features - 50
			deeplearning makes it obsolete (end to end) - 53

	classification - 54
		nearest neighbor - 54
			simple but slow - 56
		linear - 57
		decision trees / 20 questions - 1.00

	supervision , annotation - 1.02
		spectrum of supervision - 1.07

	generalization vs memory - 1.11
	diagnosing generization ability / a model's ability - 1.13
		overfitting & underfitting - 1.13
			freedom of a model - 1.14
			cross validation - 1.18
			"if you increase the size of the model, you need more data to accomodate" - 1.21
+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=
nothing today
+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=
turning friday discussion into lecture & vice versa - 10
hw1 restrictions & logistics - 11
	late day policy - 11
	prebuilt libraries - 12
	what to submit - 17
	which submission counts - 18
	
machine learning review - 19
	knobs / parameters - 20
	learnable function - 21
	supervised learning - 21
	training and testing steps - 23
	classifiers - 24
		nearest neighbor - 24
		linear - 26
			what is a parametric model - 26
	types of supervision - 27
	generalization vs memory - 29
		this set is "held out" - 29
	undefitting , overfitting - 30
	model complexity & training size & test error - 35
		more data = bigger models - 36
	validation , choosing the best model - 37
		"we dont prefer the model on the test set a lot..." - 37

midterm in early november - 42

data set history - 43

neural networks - 53
	"AI nowadays are neural networks" - 54
	how binary classifier works: variables, weights, threshold - 54
		why we transpose - 56
		why we add bias into our feature matrix - 57
	spam example - 58
		"whats a word that should have negative weight" - 1.00
		"whats a word that should have zero weight?" - 1.01
				*mic is out* - 1.01
				*mic is back* - 1.02
	geometry visualization - 1.04
		spam example - 1.04
		new example - 1.08
			"why is it 90 degrees?" - 1.09
		hyperplane - 1.10

	two-layer network architecture - 1.12
		"linear models can't solve every problem", linearly non-seperable , non-linear model - 1.13
		deep model - 1.13		
+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=
you dont need permission for late days - 10

previous lecture review - 12
	perceptron - 12
	geometric visualization - 15
	
MLP multi-layer perceptron - 17
	non-linearity / link function / activation function - 19
		we want derivable functions - 21
	hidden layers / hidden neurons - 22
		more neurons = more capacity - 23
		finding the sweet spot, over-training- 25
			"i can always use SQL for 100%!" - 25
	temperature prediction example - 26
		ground truth - 27
		cost function, objective function, loss function, energy function, error function - 27
				machine learning nowadays, "optimization" - 31
					UPS/FEDEX navigation directions example "never did left turns" - 31
			using absolute value vs squaring - 34
				lasso - 38

optimization framework, and loss minimization - 39
	argument / index of array - 40
	w, freedom - 40
	why we use 1/2 - 43
	regularization - 45

"How can we optimize?" - 46
gradient descent - 47
	most used and easy to understand - 47
	"its just the derivative" - 49
	gradients & partial derivative - 50
		gradient = vector of partial derivatives - 53
	"derivative calculates going up the hill, but we want to go down the hill..." - 55
	step size / learning rate, local minima - 56

ADAM gradient descent & automatic learning rate - 1.00

MLP multi layer perceptron - 1.01

gradient descent update rule / automatic learning rate - 1.02

"how to calculate derivatives?" - 1.04
	chain rule review - 1.08
	bringing it back into MLPs - 1.11
		"this is time consuming" - 1.11
	back propogration and the chain rule - 1.12
		example - 1.14
		forward pass - 1.20		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=
previous lecture review - 7
	hw answers will be provided - 7
	linear classifier - 7
	multi layer perceptron - 8
	non-linearity / activation / link function - 9
	optimizing the number of hidden layers - 10
		optimization framework - 10
		the 2 arguments - 12
	gradient descent algorithm - 12
		gradient vector / partial derivative vector - 14
		learning rate, ADAM - 18
		
momentum, step sizes & inertia, avoiding local minima - 21
	"the number of minimas is infinite" & little ant analogy - 28
		alchemey and black magic - 29
				
optimization is slow - 33
	imageNET dataset, resNET take 20 hours to calculate - 33
	stochastic gradient / minibatch gradient, "i dont need to use all my training data" - 34
		random sampling - 36
		mini-batch & stochastic definitions - 37
	IID independentent identically sampled - 38
	EPOCH - 39
	HHD harddrives are slow, ram is 42 - fast
	
gradient descent quick refresh - 48
	forward pass backward pass -  53
	
two types of compute - 55

history of back propogation algorithm - 1.00
	auto differentiation, facebook pytorch, google tensorflow. numpy - 1.03

initializing - 1.05
	multiple solutions = #(hidden neurons)_Factorial - 1.06
	infinite number of local minima - 1.09
	
DAG directed acyclic graph - 1.11

breadth vs depth, vanishing gradient exploding gradient - 1.12

CPU vs GPU - 1.16
		*mic goes out* - 1.17
		*mic comes back* - 1.18
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=
+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
