+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=
where slides are found - 6

past lecture quick review - 8
	why computer vision is difficult, list of challenges computers face - 11
		stereo vision spiel - 14
		context - 16

syllabus covering - 18
	"let me know if you're interseted in learning more, i can change the syllabus" - 22
	"final will be easy" , final project - 23

"have you heard of these before?" - 32

images and filters - 35
	what is an image? - 35
		rgb, grayscale - 36
		tensor, matrix, array, scalar, cubes, dimensions - 37
		an image is a cube/3d tensor (3rd dimension is color/grayscale) - 38

	images as functions - 38

	image operations - 40
			"assume everything is grayscale for this lecture" - 42
		edge detection - 43
		converting image to vector - 43
		comparing two images - 44
		denoising - 45
		
	image filtering/convolution - 46
		g-matrix - 47
		blurring/smoothing - 49
			what does it mean to convolution/convolving? - 50
			explaining the formula - 52
			handling the border of the image (0 padding) - 53
		box filter - 57
			why choose 1/9 - 57

		this filter does nothing - 59
				einsteins eye - 1.00
		left shift filter - 1.01
		sharpening filter / brightness subtracted by smoothing - 1.01
		edge detection filter vertical and horizontal - 1.05
		
		gaussian filter - 1.09
			what does the standard deviation do? variance? - 1.15
			gaussian vs box - 1.16
			combining gaussian filters - 1.18
			seperable filters are more efficient - 1.19		
+=+=+=+=+=+=+=+= 9 - 30 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=
exams and cheatsheets - 5

filters review - 8
	discretizing meaning and why we do it - 8
	pixels, cameras, RGB, displays - 9
	operations - 10
		image filtering - 11
		why our eyes our blurry, pinhole cameras, pupils & retinas - 12
			early cameras - 14
				lenses - 20
			why our eyes are blurry - 22
		other filters, edge detection, gaussian - 24

box filter algorithm / sums of rectangular regions - 26
	calculating the sum more efficiently  - 28
	"this way is efficient but its not the best" - 31
	the best way - 32
		cant be done with gaussian, and their is a big 1-time cost  - 34

addressing constant blur to remove noise without removing detail - 35
	bilateral bilter on a height field - 37
		theory and general math behind it - 39
			"big differences will have much less weight... the edge will not be smoothed out" - 40
			"this is not convolution anymore" - 42
		explaining and going deeper into math - 42
		the result of the bilateral filtering - 45
			"its not a convolution" - 48

AI image generation - 49

borders - 51

sampling - 53
	image scaling / sub-sampling / size reduction - 54
		sub sampling by using every other pixel value - 55
			interpolation/averaging - 55
			the quality drops a lot, information is lost, artifacts are introduced - 57
		sub sampling with gaussian pre-filtering introduction - 58

	fourier transform - 59
		waves visualized inside the fourier domain - 1.09
		jpg and mp3 file compression and how our ears hear - 1.11
			Nyquist and why things are sampled at 44khz when we can only hear 22khz - 1.16
			the old phone system's limitations  - 1.19
+=+=+=+=+=+=+=+= 10 - 2 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=
past lecture recap - 6
	artifacts - 6
	sub sampling - 7
	gaussian pre-filtering - 8
	fourier transform - 9
	nyquist rate sampling & aliasing - 10
		avoiding aliasing - 13

	using fourier to make an image(i think?) - 17
		fourier holds in any dimension - 17

	more on subsampling - 23
		sampling too much? - 25
		phone sample example - 27

what aliasing looks like - 29
	helicopter, car wheel example - 32
	"this is because of UNDERsampling, oversampling has no effect" - 36
		
upsampling - 37
	upsampling methods (nearest neighbor, bilinear, bicubic) - 38
	superresolution / ML for upsampling, its on your phone - 43
	
oversampling downsides - 44

edge detection - 45
	recognition is very related to edge detection - 45
	some neurons are designed for edge detection - 47
	why mimicking humans isnt ideal - 49

	what makes an edge - 50
		discontinuity (surface normal, depth, surface color, illumination) - 51
		
	finding edges using algorithms - 53
		1st, 2nd derivatives - 54

	image gradient, derivative for an image/vector / high dimension derivatives - 55
		partial derivative - 56
		"its always pointing at the whiter part" - 1.00
		
		calculate derivative of discrete/non-continuous signal - 1.03
	
	sobel operator - 1.06
	noise vs derivatives - 1.11
		Derivative Theorem of Convolution - 1.12
		canny edge detection, orientation of an edge - 1.16
+=+=+=+=+=+=+=+= 10 - 7 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=
edge detection recap - 7
	saddle point of derivative - 13
	sobel filter - 15
	Derivative Theorem of Convolution - 17

canny edge detection - 19
	hough space - 24

intro to machine learning - 33
	recognition function, y=f(x) - 34
	training vs testing - 36
		"we dont want to memorize" - 37
			self driving car example - 40

	steps to machine learning - 42
	efficiency & power consumption & edge AI - 45

	feature extraction, turning an image into a vector - 48
		some possible "obsolete" features - 50
			deeplearning makes it obsolete (end to end) - 53

	classification - 54
		nearest neighbor - 54
			simple but slow - 56
		linear - 57
		decision trees / 20 questions - 1.00

	supervision , annotation - 1.02
		spectrum of supervision - 1.07

	generalization vs memory - 1.11
	diagnosing generization ability / a model's ability - 1.13
		overfitting & underfitting - 1.13
			freedom of a model - 1.14
			cross validation - 1.18
			"if you increase the size of the model, you need more data to accomodate" - 1.21
+=+=+=+=+=+=+=+= 10 - 9 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=
nothing today
+=+=+=+=+=+=+=+= 10 - 14 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=
turning friday discussion into lecture & vice versa - 10
hw1 restrictions & logistics - 11
	late day policy - 11
	prebuilt libraries - 12
	what to submit - 17
	which submission counts - 18
	
machine learning review - 19
	knobs / parameters - 20
	learnable function - 21
	supervised learning - 21
	training and testing steps - 23
	classifiers - 24
		nearest neighbor - 24
		linear - 26
			what is a parametric model - 26
	types of supervision - 27
	generalization vs memory - 29
		this set is "held out" - 29
	undefitting , overfitting - 30
	model complexity & training size & test error - 35
		more data = bigger models - 36
	validation , choosing the best model - 37
		"we dont prefer the model on the test set a lot..." - 37

midterm in early november - 42

data set history - 43

neural networks - 53
	"AI nowadays are neural networks" - 54
	how binary classifier works: variables, weights, threshold - 54
		why we transpose - 56
		why we add bias into our feature matrix - 57
	spam example - 58
		"whats a word that should have negative weight" - 1.00
		"whats a word that should have zero weight?" - 1.01
				*mic is out* - 1.01
				*mic is back* - 1.02
	geometry visualization - 1.04
		spam example - 1.04
		new example - 1.08
			"why is it 90 degrees?" - 1.09
		hyperplane - 1.10

	two-layer network architecture - 1.12
		"linear models can't solve every problem", linearly non-seperable , non-linear model - 1.13
		deep model - 1.13		
+=+=+=+=+=+=+=+= 10 - 16 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=
you dont need permission for late days - 10

previous lecture review - 12
	perceptron - 12
	geometric visualization - 15
	
MLP multi-layer perceptron - 17
	non-linearity / link function / activation function - 19
		we want derivable functions - 21
	hidden layers / hidden neurons - 22
		more neurons = more capacity - 23
		depth vs width - 25
		finding the sweet spot, over-training - 25
			"i can always use SQL for 100%!" - 25
	temperature prediction example - 26
		ground truth - 27
		cost function, objective function, loss function, energy function, error function - 27
				machine learning nowadays, "optimization" - 31
					UPS/FEDEX navigation directions example "never did left turns" - 31
			using absolute value vs squaring - 34
				lasso - 38

optimization framework, and loss minimization - 39
	argument / index of array - 40
	w, freedom - 40
	why we use 1/2 - 43
	regularization - 45

"How can we optimize?" - 46
gradient descent - 47
	most used and easy to understand - 47
	"its just the derivative" - 49
	gradients & partial derivative - 50
		gradient = vector of partial derivatives - 53
	"derivative calculates going up the hill, but we want to go down the hill..." - 55
	step size / learning rate, local minima - 56

ADAM gradient descent & automatic learning rate - 1.00

MLP multi layer perceptron - 1.01

gradient descent update rule / automatic learning rate - 1.02

"how to calculate derivatives?" - 1.04
	chain rule review - 1.08
	bringing it back into MLPs - 1.11
		"this is time consuming" - 1.11
	back propogation and the chain rule - 1.12
		example - 1.14
		forward pass - 1.20		
+=+=+=+=+=+=+=+= 10 - 21 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
		
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=
previous lecture review - 7
	hw answers will be provided - 7
	linear classifier - 7
	multi layer perceptron - 8
	non-linearity / activation / link function - 9
	optimizing the number of hidden layers - 10
		optimization framework - 10
		the 2 arguments - 12
	gradient descent algorithm - 12
		gradient vector / partial derivative vector - 14
		learning rate, ADAM - 18
		
momentum, step sizes & inertia, avoiding local minima - 21
	"the number of minimas is infinite" & little ant analogy - 28
		alchemey and black magic - 29
				
optimization is slow - 33
	imageNET dataset, resNET take 20 hours to calculate - 33
	stochastic gradient / minibatch gradient, "i dont need to use all my training data" - 34
		random sampling - 36
		mini-batch & stochastic definitions - 37
	IID independentent identically sampled - 38
	EPOCH - 39
	HDD harddrives are slow, ram is 42 - fast
	
gradient descent quick refresh - 48
	forward pass backward pass -  53
	
two types of compute - 55

history of back propogation algorithm - 1.00
	auto differentiation, facebook pytorch, google tensorflow. numpy - 1.03

initializing - 1.05
	multiple solutions = #(hidden neurons)_Factorial - 1.06
	infinite number of local minima - 1.09
	
DAG directed acyclic graph - 1.11

breadth vs depth, vanishing gradient exploding gradient - 1.12

CPU vs GPU - 1.16
		*mic goes out* - 1.17
		*mic comes back* - 1.18
+=+=+=+=+=+=+=+= 10 - 23 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=
no numerical examples for gradient descent - 6
hw2, gradient descent - 7

previous lecture review - 8
	dynamic programming - 9
	momentum - 9
	
activation functions - 12
	why dont we use sign function - 12
		sign vs step vs sine (lol whoever asked, thank you) - 14
	sigmoid, tan, and vanishing gradient, saturation - 15
	ReLu function, rectified linear unit - 17
		but derivative = 0 here - 18
			leaky ReLu - 18
				"doesnt help much" - 19
		ReLu is better than sigmoid - 19
			"what about this non-differentiable point?", engineer vs math approach - 20
			
convolutional neural networks CNN - 22
	end to end learning - 23
	MLP 3d weight visualization - 26
	
tensorflow playground - 27
	linear cannot solve this problem - 29
	small,large learning rate - 31
	epoch - 32
		"usually we do 50 epoch, each epoch takes 20 minutes" - 32
	large step sizes - 34
		"it converges but still fluctuates" - 34
		"theres no benefit to large step sizes" - 34
	"how do you know you're done?" - 35
		there is automation but todays we mostly use babysitting - 36
	ReLu vs sigmoid - 36
		ReLu is a polygonal, and it can never smooth out the corners - 37
	regularization, L1, L2, absolute value, lasso, sparse - 38
		L1 vs L2 - 41
			lambda regularization rate - 43
			capacity reduction - 46
				sparse - 47
	adding hidden layers, neurons - 50
		"if its failing on training, the model is lacking capacity" - 51
		breadth vs depth, "more layers is better but problematic" - 51
	adding input - 52
	why the model isnt a perfect spiral - 53
		*mic out* - 53
		*mic in* - 55
	classification vs regression/prediction - 55
		"sometimes extra input can be confusing" - 57
	adding noise - 57
	bach size, small vs large - 58
		bach size and training time - 1.00
			mini bach has more updates which is better - 1.00
			cons of mini bach - 1.03
		even though this one is 20k+ epochs to solve, its because our input is small - 1.06
			half epochs - 1.06

CNN is an MLP with convolutions instead of linear layers - 1.09
	tensor review - 1.09
	weight mask, image, feature map, channels, convolutional layer, fully connected, & calculating their sizes - 1.10 
	feature extraction - 1.16
		low,mid,high level - 1.19
	flow of CNN - 1.19
		spatial pooling, max pooling, stride - 1.21
+=+=+=+=+=+=+=+= 10 - 28 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 10 - 30 thursday +=+=+=+=+=+=+=+=
swift coding club - 6

past lecture review - 8
	combining convolutions while avoiding linearity - 9
	ReLu, avoiding saturation - 11
	spatial pooling, stride, "reduce the amount of info we pass forward" - 18
	
boat cnn example - 21
	fully connected meaning - 
	every layer has an activation after it - 23
	softmax activation fubction, probability distribution - 25
	long vector dimension, vectorization - 
	"how i analyze a research paper" - 30
	1-hot encoding - 31
	dimension of tensor,vector - 32
	"maybe i should stop recording" - 35
	calculating loss - 38
	"it must be differentiable" - 44
	non differentiable and reinforcement learning - 45
	not using fully connected for convolution, "detect the cat anywhere in the image" - 49
	
SIFT, AlexNet, deep learning, end to end, & history - 51
	lynette's digit detection - 53
	AlexNet and using GPU - 55
	AlexNet changes the game - 1.04
	
looking inside what the model sees, explainable AI, top 9 patches - 1.09
	layer 1 - 1.09
		how to see the top filters - 1.10
	layer 2 - 1.14
	layer 3 - 1.15
	layer 4 - 1.16
		"why are there so many dogs?" - 1.16 
	layer 5 - 1.17
	why is explainable AI important, medical - 1.20
	
error top 5, top 5 error - 1.22
+=+=+=+=+=+=+=+= 10 - 30 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 4 tuesday +=+=+=+=+=+=+=+=
max pooling stride - 7
	its still used today even though it doesnt improve speed - 11
	
final project proposal - 12

midterm, "dont worry about it" - 13	

previous lecture review - 14
	top 9 patches , what the model sees - 14
	tweaking pre-built models - 19
	early exit - 19
	"why would we want to reduce size?", edge devices, edge computing - 21
	
imageNet challenge - 22
	top 5 error - 24
	"human expert is stupid" - 27
	comptetition cheating - 29
			*mic out* - 32
			*mic in* - 33
			
Resnet "going beyond 22 layers was difficult"  - 34
	"if one layer doesnt work, the chain breaks" - 36
	skip connection - 37
	average pooling - 39
	"why not one connection from the input all the way to the output - 42
	resnet block - 42
	
	DenseNet - 43
	Neural Architectural Search, NAS, AI improving model architecture - 44
	
	"why not a connection for every block instead of every other block?" - 45
	skip connections are differentiable - 46
	effective depth - 48
	plain network vs resnet comparison - 50
	
BN batch normalization, handling the constant changing of layer inputs/outputs - 52
	"normalizing the distribution" - 55
	its several times faster - 55
	parameters mew sigma / gamma beta - 58
		"you can take this extra stuff out and be fine" - 1.00
	"Do i need to keep this for the final model?", baking in parameters into the model weight and bias - 1.01

deep learning packages & history: cafe, cuda, torch, lua - 1.08
	cuda, nvidia, amd - 1.16
		how companies get more customers - 1.18
	
coding on the midterm - 1.20
	double sided hand written cheatsheet - 1.22
+=+=+=+=+=+=+=+= 11 - 4 tuesday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================

+=+=+=+=+=+=+=+= 11 - 6 thursday +=+=+=+=+=+=+=+=
past lecture review - 4
	top-9 patches, imagenet challenge - 4
	batch norm - 5
		layer norm instead of batch norm - 7
	resnet - 9
	deep learning packages, pytorch - 10
	
adversarial, breaking CNNs - 12
	designing adversarial - 13
		black box, white box - 15
	targeted vs no target attack - 17
	modified gradient, "going up the hill not down the hill" - 18
	
	"this isnt practical", adversarial patch - 19
		hillary clinton glasses - 24
		defense against adversarial patch - 27
		
	simple solution, add adversarial images to training, adversarial training, adversarial example - 28
		"its not the best solution" - 30
		*mic out* - 30
		*mic in* - 31
	sharpness aware minimization - 31

data augmentation (cropping, scale, flipping, angling...) - 36
	"if you dont do augmentation in DNN, you will overfit" - 39
	pytorch & augmentation - 40
	we dont test on augmented images - 41
	be careful of augmentation leaking from training to testing - 41
	
	augmentation is domain specific - 42
			augmentation in test time, 10crop - 43
		examples of when augmentation can go wrong,right - 43
		
dropout - 45
	dont use it during test time - 48
	pruning, permanently removing inactive neurons - 49
	drop connect - 50
	
model ensemble, "why are there 30 people playing the violin when 1 would be fine?" - 51

large datasets dont use augmentation - 55

"augmentation should be done on the fly" - 56

transfer learning, building models off small data sets - 57
	frozen layers- 58
	how it works, "feature learned through imagenet may be sufficient..." - 59
	
	the initial data set should be diverse, imagenet has many applications - 1.01
	
	"this visualization isnt that great" - 1.03
	
	the more data you have, the more of the model you train - 1.04
		fine tuning - 1.05
	pre training - 1.08
	CNN comparison - 1.09
	
	transfer learning for image captions, open vocabulary, closed vocabulary - 1.11
	
parallel computation, hadoop, mapreduce - 1.14
	gpu failure - 1.18
model parallel, weight parallel, data parallel - 1.22
+=+=+=+=+=+=+=+= 11 - 6 thursday +=+=+=+=+=+=+=+=

================================================================================================================================
================================================================================================================================
